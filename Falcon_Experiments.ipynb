{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41faab6f-4054-478d-98a2-70aa9d32b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import Utility as util\n",
    "from Utility import REDataset_entities, REModelWithAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca0f13f-499c-41ca-8fff-e93b07baa714",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewRel = pd.read_csv('fewRel_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0923ddc7-08c6-4467-a935-fdac37c5ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_falcon = pd.read_csv('aug_falcon_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee551cea-c64a-481d-b001-b430ea394e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_falcon = util.sampleData(df_aug_falcon,'relation') # this code is for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f13e9fcb-3ec6-43f8-a7a6-b993c58599a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orignal dataset = 44800 and augumented data = 3200\n"
     ]
    }
   ],
   "source": [
    "print(f'orignal dataset = {len(df_fewRel)} and augumented data = {len(df_aug_falcon)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ca043a9-9319-4097-8aeb-f5db75f17934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set = 35840 and test set = 8960\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df_fewRel, test_size=0.2, random_state=42) # To take test set from orignal data\n",
    "print(f'Train set = {len(train_df)} and test set = {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36eee20d-54a8-4b47-9099-12183808e5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39040\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([train_df, df_aug_falcon], ignore_index=True) # combined augumented data with orignal FewRel training data\n",
    "print(len(df_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0af25083-236d-4a12-adb7-c19dfd2ff0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['entities'])\n",
    "test_df = test_df.drop(columns=['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "894baca2-a2f0-4467-b16c-18a5a945ac01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode training and test set for relation\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['relation_id'] = label_encoder.fit_transform(train_df['relation'])\n",
    "test_df['relation_id'] = label_encoder.fit_transform(test_df['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfb9873-3a15-4485-b680-183b5ae8bcf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b0bf69a-0b1c-47df-b8f0-4dfff8783ebb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose the same model as in your model\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f8eaa1d-c6e7-42fb-a57e-849a3a085c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  \n",
    "train_dataset = util.REDataset(train_df,tokenizer, max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = util.REDataset(test_df, tokenizer, max_seq_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ad53bff-ca15-4e6e-99e9-3ea40af08fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = REModelWithAttention(tokenizer, num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db1a6a4-df73-4627-843e-f32d2e2a734a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.2417- Validation Loss: 1.3332\n",
      "Epoch [2/20] - Training Loss: 1.1481- Validation Loss: 1.0907\n",
      "Epoch [3/20] - Training Loss: 0.7950- Validation Loss: 1.0404\n",
      "Epoch [4/20] - Training Loss: 0.5755- Validation Loss: 1.0393\n",
      "Epoch [5/20] - Training Loss: 0.4227- Validation Loss: 1.0597\n",
      "Epoch [6/20] - Training Loss: 0.3233- Validation Loss: 1.1274\n",
      "Epoch [7/20] - Training Loss: 0.2529- Validation Loss: 1.1750\n",
      "Epoch [8/20] - Training Loss: 0.2034- Validation Loss: 1.2572\n",
      "Early stopping after 8 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # 7 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ddf0b37-eb66-4afd-9e35-49cadc66b554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7079\n",
      "Precision: 0.7132\n",
      "Recall: 0.7059\n",
      "F1 Score: 0.7055\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader) # 7 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fa511527-efd4-4fb1-b9b5-83808b10965d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.1620- Validation Loss: 1.3198\n",
      "Epoch [2/20] - Training Loss: 1.1457- Validation Loss: 1.1031\n",
      "Epoch [3/20] - Training Loss: 0.8042- Validation Loss: 1.0131\n",
      "Epoch [4/20] - Training Loss: 0.5729- Validation Loss: 1.0141\n",
      "Epoch [5/20] - Training Loss: 0.4173- Validation Loss: 1.0388\n",
      "Epoch [6/20] - Training Loss: 0.3083- Validation Loss: 1.0921\n",
      "Epoch [7/20] - Training Loss: 0.2330- Validation Loss: 1.1676\n",
      "Early stopping after 7 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # 3 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "084dcd2d-ac1a-4148-90a4-687735d53037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7213\n",
      "Precision: 0.7215\n",
      "Recall: 0.7201\n",
      "F1 Score: 0.7167\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # 3 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db74a443-8eea-4719-b4cb-3c2d8f324fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.1018- Validation Loss: 1.3351\n",
      "Epoch [2/20] - Training Loss: 1.0879- Validation Loss: 1.0731\n",
      "Epoch [3/20] - Training Loss: 0.7618- Validation Loss: 0.9812\n",
      "Epoch [4/20] - Training Loss: 0.5373- Validation Loss: 1.0099\n",
      "Epoch [5/20] - Training Loss: 0.3895- Validation Loss: 1.0136\n",
      "Epoch [6/20] - Training Loss: 0.2866- Validation Loss: 1.0684\n",
      "Epoch [7/20] - Training Loss: 0.2177- Validation Loss: 1.1077\n",
      "Early stopping after 7 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # Orignal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f86ac1-bc28-416f-9bd5-f0124bd28688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7215\n",
      "Precision: 0.7239\n",
      "Recall: 0.7191\n",
      "F1 Score: 0.7169\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # orignal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4a033cb-f5fd-47e8-9902-9f1cb9c4333d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.2260- Validation Loss: 1.3595\n",
      "Epoch [2/20] - Training Loss: 1.1679- Validation Loss: 1.1595\n",
      "Epoch [3/20] - Training Loss: 0.8275- Validation Loss: 1.1447\n",
      "Epoch [4/20] - Training Loss: 0.6153- Validation Loss: 1.2201\n",
      "Epoch [5/20] - Training Loss: 0.4624- Validation Loss: 1.2791\n",
      "Epoch [6/20] - Training Loss: 0.3590- Validation Loss: 1.3769\n",
      "Epoch [7/20] - Training Loss: 0.2815- Validation Loss: 1.4314\n",
      "Early stopping after 7 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # without entities 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9c822687-4c3e-4f9a-a5e7-9efedf76b018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6616\n",
      "Precision: 0.6609\n",
      "Recall: 0.6601\n",
      "F1 Score: 0.6539\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # without entities 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eda370e8-1702-493f-a5c4-6fd983fdbc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.1552- Validation Loss: 1.3565\n",
      "Epoch [2/20] - Training Loss: 1.1240- Validation Loss: 1.1456\n",
      "Epoch [3/20] - Training Loss: 0.8066- Validation Loss: 1.1516\n",
      "Epoch [4/20] - Training Loss: 0.5917- Validation Loss: 1.1909\n",
      "Epoch [5/20] - Training Loss: 0.4344- Validation Loss: 1.2529\n",
      "Epoch [6/20] - Training Loss: 0.3204- Validation Loss: 1.3646\n",
      "Early stopping after 6 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # without entities orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2059d48f-bad1-4e2e-9da1-56d814d10ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6675\n",
      "Precision: 0.6655\n",
      "Recall: 0.6655\n",
      "F1 Score: 0.6614\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # without entities orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8721145a-3593-47fc-bda9-8a9dee7b5b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader,device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, targets in data_loader:\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff557615-b5c0-4b5e-b8b9-f23f03768386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b15ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import Utility as util\n",
    "from Utility import REDataset_entities, REModelWithAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0140d7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fewRel = pd.read_csv('fewRel_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9e1904",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_falcon = pd.read_csv('aug_falcon_entities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "871ac800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug_falcon = util.sampleData(df_aug_falcon,'relation') # this code is for sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f541f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orignal dataset = 44800 and augumented data = 3200\n"
     ]
    }
   ],
   "source": [
    "print(f'orignal dataset = {len(df_fewRel)} and augumented data = {len(df_aug_falcon)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce6947f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set = 35840 and test set = 8960\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(df_fewRel, test_size=0.2, random_state=42) # To take test set from orignal data\n",
    "print(f'Train set = {len(train_df)} and test set = {len(test_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c220d15f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39040\n"
     ]
    }
   ],
   "source": [
    "df_combined = pd.concat([train_df, df_aug_falcon], ignore_index=True) # combined augumented data with orignal FewRel training data\n",
    "print(len(df_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc2a71b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['entities'])\n",
    "test_df = test_df.drop(columns=['entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2562b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode training and test set for relation\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['relation_id'] = label_encoder.fit_transform(train_df['relation'])\n",
    "test_df['relation_id'] = label_encoder.fit_transform(test_df['relation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7ed109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33e7e14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Choose the same model as in your model\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4274fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 128  \n",
    "train_dataset = util.REDataset(train_df,tokenizer, max_seq_length)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "test_dataset = util.REDataset(test_df, tokenizer, max_seq_length)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a076a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = REModelWithAttention(tokenizer, num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "# Define your loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5aab8ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.2417- Validation Loss: 1.3332\n",
      "Epoch [2/20] - Training Loss: 1.1481- Validation Loss: 1.0907\n",
      "Epoch [3/20] - Training Loss: 0.7950- Validation Loss: 1.0404\n",
      "Epoch [4/20] - Training Loss: 0.5755- Validation Loss: 1.0393\n",
      "Epoch [5/20] - Training Loss: 0.4227- Validation Loss: 1.0597\n",
      "Epoch [6/20] - Training Loss: 0.3233- Validation Loss: 1.1274\n",
      "Epoch [7/20] - Training Loss: 0.2529- Validation Loss: 1.1750\n",
      "Epoch [8/20] - Training Loss: 0.2034- Validation Loss: 1.2572\n",
      "Early stopping after 8 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # 7 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4e3405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7079\n",
      "Precision: 0.7132\n",
      "Recall: 0.7059\n",
      "F1 Score: 0.7055\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader) # 7 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6476231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.1620- Validation Loss: 1.3198\n",
      "Epoch [2/20] - Training Loss: 1.1457- Validation Loss: 1.1031\n",
      "Epoch [3/20] - Training Loss: 0.8042- Validation Loss: 1.0131\n",
      "Epoch [4/20] - Training Loss: 0.5729- Validation Loss: 1.0141\n",
      "Epoch [5/20] - Training Loss: 0.4173- Validation Loss: 1.0388\n",
      "Epoch [6/20] - Training Loss: 0.3083- Validation Loss: 1.0921\n",
      "Epoch [7/20] - Training Loss: 0.2330- Validation Loss: 1.1676\n",
      "Early stopping after 7 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # 3 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18b6405b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7213\n",
      "Precision: 0.7215\n",
      "Recall: 0.7201\n",
      "F1 Score: 0.7167\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # 3 % with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80514d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.1018- Validation Loss: 1.3351\n",
      "Epoch [2/20] - Training Loss: 1.0879- Validation Loss: 1.0731\n",
      "Epoch [3/20] - Training Loss: 0.7618- Validation Loss: 0.9812\n",
      "Epoch [4/20] - Training Loss: 0.5373- Validation Loss: 1.0099\n",
      "Epoch [5/20] - Training Loss: 0.3895- Validation Loss: 1.0136\n",
      "Epoch [6/20] - Training Loss: 0.2866- Validation Loss: 1.0684\n",
      "Epoch [7/20] - Training Loss: 0.2177- Validation Loss: 1.1077\n",
      "Early stopping after 7 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # Orignal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bc0e7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7215\n",
      "Precision: 0.7239\n",
      "Recall: 0.7191\n",
      "F1 Score: 0.7169\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # orignal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c04915d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.2260- Validation Loss: 1.3595\n",
      "Epoch [2/20] - Training Loss: 1.1679- Validation Loss: 1.1595\n",
      "Epoch [3/20] - Training Loss: 0.8275- Validation Loss: 1.1447\n",
      "Epoch [4/20] - Training Loss: 0.6153- Validation Loss: 1.2201\n",
      "Epoch [5/20] - Training Loss: 0.4624- Validation Loss: 1.2791\n",
      "Epoch [6/20] - Training Loss: 0.3590- Validation Loss: 1.3769\n",
      "Epoch [7/20] - Training Loss: 0.2815- Validation Loss: 1.4314\n",
      "Early stopping after 7 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # without entities 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "695af7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6616\n",
      "Precision: 0.6609\n",
      "Recall: 0.6601\n",
      "F1 Score: 0.6539\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # without entities 7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0767f1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Training Loss: 2.1552- Validation Loss: 1.3565\n",
      "Epoch [2/20] - Training Loss: 1.1240- Validation Loss: 1.1456\n",
      "Epoch [3/20] - Training Loss: 0.8066- Validation Loss: 1.1516\n",
      "Epoch [4/20] - Training Loss: 0.5917- Validation Loss: 1.1909\n",
      "Epoch [5/20] - Training Loss: 0.4344- Validation Loss: 1.2529\n",
      "Epoch [6/20] - Training Loss: 0.3204- Validation Loss: 1.3646\n",
      "Early stopping after 6 epochs without improvement.\n"
     ]
    }
   ],
   "source": [
    "train_loss, valid_loss = util.train(model, train_loader, test_loader, criterion, optimizer, device, patience=4, num_epochs=20) # without entities orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4f02812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6675\n",
      "Precision: 0.6655\n",
      "Recall: 0.6655\n",
      "F1 Score: 0.6614\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, test_loader,device) # without entities orignal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cc811ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader,device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, targets in data_loader:\n",
    "            input_ids, attention_mask, targets = input_ids.to(device), attention_mask.to(device), targets.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            total_samples += targets.size(0)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    average_loss = total_loss / len(data_loader)\n",
    "\n",
    "    accuracy = accuracy_score(all_targets, all_predictions)\n",
    "    precision = precision_score(all_targets, all_predictions, average='macro')\n",
    "    recall = recall_score(all_targets, all_predictions, average='macro')\n",
    "    f1 = f1_score(all_targets, all_predictions, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125bfe60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Losses: [1.592, 0.4043, 0.2274, 0.1438, 0.0939, 0.0646, 0.0483]\n",
      "Validation Losses: [0.5885, 0.4213, 0.4154, 0.4207, 0.4386, 0.5011, 0.5251]\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"\n",
    "Epoch [1/20] - Training Loss: 1.5920- Validation Loss: 0.5885\n",
    "Epoch [2/20] - Training Loss: 0.4043- Validation Loss: 0.4213\n",
    "Epoch [3/20] - Training Loss: 0.2274- Validation Loss: 0.4154\n",
    "Epoch [4/20] - Training Loss: 0.1438- Validation Loss: 0.4207\n",
    "Epoch [5/20] - Training Loss: 0.0939- Validation Loss: 0.4386\n",
    "Epoch [6/20] - Training Loss: 0.0646- Validation Loss: 0.5011\n",
    "Epoch [7/20] - Training Loss: 0.0483- Validation Loss: 0.5251\n",
    "\"\"\"\n",
    "\n",
    "# Initialize empty lists for training and validation losses\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "\n",
    "# Split the data into lines and iterate through them\n",
    "for line in data.strip().split('\\n'):\n",
    "    # Split each line into words\n",
    "    words = line.split()\n",
    "    \n",
    "    # Extract training and validation losses and append them to the respective lists\n",
    "    training_loss = float(words[5].split('-')[0])\n",
    "    validation_loss = float(words[-1])\n",
    "    \n",
    "    training_losses.append(training_loss)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "# Print the extracted lists\n",
    "print(\"Training Losses:\", training_losses)\n",
    "print(\"Validation Losses:\", validation_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef5f447-edc7-497c-868e-830794640731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
